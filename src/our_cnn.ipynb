{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code based on:\n",
    "* https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html\n",
    "* https://pytorch.org/tutorials/beginner/introyt/trainingyt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import data_utils.data_loading as data_load\n",
    "from data_utils.cifar10_testset import CIFAR10Testset\n",
    "from custom_nets.vgglike import VGGLike\n",
    "from custom_nets.resnetlike import ResNetLike, ResBlock\n",
    "from data_utils.data_preparation import Cutout\n",
    "\n",
    "random_seed = 42\n",
    "random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTrainer:\n",
    "\n",
    "    def __init__(self, batch_size: int, device: str, load_train_from_torch=False):\n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.trainloader, self.valloader = self.load_data(load_from_torch=load_train_from_torch)\n",
    "        if not os.path.exists('../nets'):\n",
    "            os.makedirs('../nets')\n",
    "        if not os.path.exists('../loss'):\n",
    "            os.makedirs('../loss')\n",
    "        if not os.path.exists('../accuracy'):\n",
    "            os.makedirs('../accuracy')\n",
    "  \n",
    "    def load_data(self, load_from_torch):\n",
    "        \n",
    "        if not load_from_torch:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "            ])\n",
    "            return data_load.load_train_data(transform, self.batch_size)\n",
    "        else:\n",
    "            transform = transforms.Compose([\n",
    "                transforms.Pad(4),\n",
    "                transforms.RandomHorizontalFlip(),\n",
    "                transforms.RandomCrop(32),\n",
    "                transforms.ToTensor(),\n",
    "                Cutout(2, 10, p=0.4)])\n",
    "            return data_load.load_torch_train_data(transform, self.batch_size)\n",
    "        \n",
    "    def train_one_epoch(self, network: nn.Module, optimizer, loss_criterion, epoch: int):\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        last_loss = 0.0\n",
    "\n",
    "        running_accuracy = 0.0\n",
    "        last_accuracy = 0.0\n",
    "\n",
    "        for i, data in enumerate(self.trainloader):\n",
    "            \n",
    "            inputs, labels = data[0].to(self.device), data[1].to(self.device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = network(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            loss = loss_criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            running_accuracy += (labels == predicted).sum().item() / len(labels)\n",
    "\n",
    "            del inputs, labels, outputs\n",
    "\n",
    "            if i % 200 == 199:\n",
    "                last_loss = running_loss / 200\n",
    "                last_accuracy = running_accuracy / 200\n",
    "                print(f'[epoch: {epoch + 1}, batches: {i - 198:5d} - {i + 1:5d}] train loss: {last_loss:.3f}, train accuracy: {last_accuracy:.3f}')\n",
    "                running_loss = 0.0\n",
    "                running_accuracy = 0.0\n",
    "\n",
    "        return last_loss, last_accuracy\n",
    "\n",
    "\n",
    "    def train_network(self, network: nn.Module, optimizer, loss_criterion, number_of_epochs: int, name: str):\n",
    "        \n",
    "        network.to(self.device)\n",
    "        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        best_networkstate_path = f'../nets/cifar_{name}_{format(timestamp)}'\n",
    "        best_vloss = 1_000_000.\n",
    "        loss = np.empty((2, number_of_epochs))\n",
    "        accuracy = np.empty((2, number_of_epochs))\n",
    "\n",
    "        for epoch in range(number_of_epochs):\n",
    "            loss[0, epoch], accuracy[0, epoch] = self.train_one_epoch(network, optimizer, loss_criterion, epoch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                running_vloss = 0.0\n",
    "                running_vaccuracy = 0.0\n",
    "                for i, vdata in enumerate(self.valloader):\n",
    "                    vinputs, vlabels = vdata[0].to(self.device), vdata[1].to(self.device)\n",
    "                    voutputs = network(vinputs)\n",
    "                    _, vpredicted = torch.max(voutputs.data, 1)\n",
    "                    vloss = loss_criterion(voutputs, vlabels)\n",
    "                    running_vloss += vloss\n",
    "                    running_vaccuracy += (vlabels == vpredicted).sum().item() / len(vlabels)\n",
    "                    del vinputs, vlabels, voutputs\n",
    "\n",
    "            loss[1, epoch] = avg_vloss = running_vloss / (i + 1)\n",
    "            accuracy[1, epoch] = avg_vaccuracy = running_vaccuracy / (i + 1)\n",
    "            print(f'[epoch: {epoch + 1}] validation loss: {avg_vloss:.3f}, validation accuracy: {avg_vaccuracy:.3f}')\n",
    "            np.savetxt(f'../loss/cifar_{name}_{format(timestamp)}.csv', loss[:, :(epoch + 1)], delimiter=',')\n",
    "            np.savetxt(f'../accuracy/cifar_{name}_{format(timestamp)}.csv', accuracy[:, :(epoch + 1)], delimiter=',')\n",
    "\n",
    "            if avg_vloss < best_vloss:\n",
    "                best_vloss = avg_vloss\n",
    "                torch.save(network.state_dict(), best_networkstate_path)\n",
    "\n",
    "        print('Finished Training')\n",
    "        self.visualize_loss(number_of_epochs, loss)\n",
    "        self.visualize_accuracy(number_of_epochs, accuracy)\n",
    "        return best_networkstate_path\n",
    "\n",
    "\n",
    "    def visualize_loss(self, number_of_epochs, loss):\n",
    "        \n",
    "        plt.plot(range(1, number_of_epochs + 1), loss[0,:], marker='o')\n",
    "        plt.plot(range(1, number_of_epochs + 1), loss[1,:], marker='o')\n",
    "        plt.legend(['train', 'validation'])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xticks(range(1, number_of_epochs + 1))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    def visualize_accuracy(self, number_of_epochs, accuracy):\n",
    "        \n",
    "        plt.plot(range(1, number_of_epochs + 1), accuracy[0,:], marker='o')\n",
    "        plt.plot(range(1, number_of_epochs + 1), accuracy[1,:], marker='o')\n",
    "        plt.legend(['train', 'validation'])\n",
    "        plt.xlabel('epoch')\n",
    "        plt.ylabel('accuracy')\n",
    "        plt.xticks(range(1, number_of_epochs + 1))\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetworkTester:\n",
    "    def __init__(self, batch_size: int, device: str):  \n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "        self.testloader, self.number_of_images = self.load_data()\n",
    "        if not os.path.exists('../tests'):\n",
    "            os.makedirs('../tests')\n",
    "\n",
    "    def load_data(self):\n",
    "        \n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        return data_load.load_test_data(transform, self.batch_size)\n",
    "    \n",
    "    def test_network(self, network: nn.Module, best_networkstate_path: str):\n",
    "        \n",
    "        classes = np.empty((self.number_of_images), dtype=object)\n",
    "        network.load_state_dict(torch.load(best_networkstate_path))\n",
    "        network.to(self.device)\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in self.testloader:\n",
    "                images = data[0].to(self.device)\n",
    "                outputs = network(images)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                predicted = predicted.tolist()\n",
    "                for predicted_label in predicted:\n",
    "                    classes[total] = CIFAR10Testset.label_number_to_class[predicted_label]\n",
    "                    total += 1\n",
    "\n",
    "        pd.DataFrame(classes, index=range(1, total + 1), columns=['label']).to_csv(best_networkstate_path.replace('nets', 'tests') + '.csv', index_label='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams():\n",
    "  def __init__(self, learning_rate, optimizer_name, weight_decay, dropout_p=0):\n",
    "    self.learning_rate = learning_rate\n",
    "    self.optimizer_name = optimizer_name\n",
    "    self.weight_decay = weight_decay\n",
    "    self.dropout_p = dropout_p\n",
    "\n",
    "  def get_optimizer(self, network):\n",
    "    if(self.optimizer_name == 'ADAM'):\n",
    "      return optim.Adam(network.parameters(), lr = self.learning_rate, weight_decay=self.weight_decay)\n",
    "    elif(self.optimizer_name == 'SGD'):\n",
    "      return optim.SGD(network.parameters(), lr=self.learning_rate, weight_decay=self.weight_decay)\n",
    "\n",
    "  def get_network_params_name(self, network):\n",
    "    return f'{network.name}_lr_{self.learning_rate}_o_{self.optimizer_name}_wd_{self.weight_decay}_d_{self.dropout_p}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGGlike custom network training and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 20\n",
    "\n",
    "hyperparams = Hyperparams(\n",
    "    learning_rate=0.001,\n",
    "    optimizer_name='ADAM',\n",
    "    weight_decay=0,\n",
    "    dropout_p=0.3\n",
    ")\n",
    "batch_size = 32\n",
    "\n",
    "trainer = NetworkTrainer(batch_size, device)\n",
    "network = VGGLike(dropout_p=hyperparams.dropout_p)\n",
    "optimizer = hyperparams.get_optimizer(network)\n",
    "save_name = hyperparams.get_network_params_name(network)\n",
    "\n",
    "best_state_path = trainer.train_network(network, optimizer, nn.CrossEntropyLoss(), number_of_epochs, save_name)\n",
    "\n",
    "tester = NetworkTester(batch_size, device)\n",
    "tester.test_network(VGGLike(dropout_p=hyperparams.dropout_p), best_state_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNetlike custom network training and tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_epochs = 20\n",
    "\n",
    "hyperparams = Hyperparams(\n",
    "    learning_rate=0.001,\n",
    "    optimizer_name='ADAM',\n",
    "    weight_decay=0.001\n",
    ")\n",
    "batch_size = 32\n",
    "\n",
    "trainer = NetworkTrainer(batch_size, device, load_train_from_torch=False)\n",
    "network = ResNetLike(ResBlock, [2, 2, 2]).to(device)\n",
    "optimizer = hyperparams.get_optimizer(network)\n",
    "save_name = hyperparams.get_network_params_name(network)\n",
    "\n",
    "best_state_path = trainer.train_network(network, optimizer, nn.CrossEntropyLoss(), number_of_epochs, save_name)\n",
    "\n",
    "tester = NetworkTester(batch_size, device)\n",
    "tester.test_network(ResNetLike(ResBlock, [2, 2, 2]).to(device), best_state_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65ca027354edb65088c08323aea4371ca72c2ad7286aa4db80ac95210aa64c56"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dl_cnn')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
